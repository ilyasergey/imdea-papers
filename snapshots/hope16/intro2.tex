\section{Talk Proposal}
\label{sc:intro} 
   
Formal verification of concurrent objects commonly requires reasoning
about linearizability~\cite{Herlihy-Wing:TOPLAS90}. This is a standard
correctness criterion whereby a concurrent execution of an object's
procedures is proved equivalent, via a simulation argument, to some
sequential execution. The clients of the object can be verified under
the sequentiality assumption, rather than by inlining the procedures
and considering their interleavings. Linearizability is often
established by describing the \emph{linearization points} (LP) of the
object, which are points in time where procedures take place,
\emph{logically}.  In other words, even if the procedure physically
executes across a time interval, exhibiting its linearization point
enables one to pretend, for reasoning purposes, that it occurred
instantaneously; hence, an interleaved execution of a number of
procedures can be reduced to a sequence of instantaneous events.

%This simplifies the reasoning about client programs, which can treat
%the physical operation as if it were indivisible, i.e.~\emph{atomic}.

However, reasoning with linearizability, and about linearization
points, can be tricky. Many times, a linearization point of a
procedure is not \emph{local}, but may appear in another procedure or
thread. Equally bad, a linearization points' place in time may not be
determined statically, but may vary based on the past, and even
future, \emph{run-time} information. This complicates the simulation
arguments, leading to unwieldy formal logical proofs.

%n a setting of formal logical proofs, often requires formal
%abstractions such as speculation or prophecy
%variables~\cite{Abadi-Lamport:LICS88}, that make proofs unwieldy.

In this talk, we present a novel specification and verification method
for concurrent objects, based on Hoare logic. It achieves the same
goal as linearizability; that is, client proofs can reason out of
Hoare triples of the object's procedures, rather than inline the
procedures and consider all interleavings. The method improves on
linearizability by offering natural abstractions for specifying
dynamic and non-local linearization points, based on familiar concepts
from Hoare logics for shared-memory concurrency. It also permits
better \emph{information hiding}, allowing us to expose to clients
less information about the internals of the object compared to
linearizability.

%For example, linearizability proofs by definition have to operate on
%the procedure's beginning and ending times. Our proofs do the same
%\emph{internally}, but most of that information can be hidden from the
%clients, which only need to see the \emph{relative logical ordering}
%of events, and then only of \emph{some} of the events.

More specifically, the method consists of two
components. First, we use shared \emph{auxiliary
  state}~\cite{Owicki-Gries:CACM76} to record, as a list of timed
events (e.g., writes occurring at a given time), the logical order in which the object's procedures are
perceived to execute, each instantaneously. Tracking this time-related
information through state enables us to specify the linearization
points \emph{dynamically}. In particular, we can use auxiliary
  code to mutate the logical order in place, thereby permuting
the sequencing of the procedures, as may be needed when some run-time
event occurs. This mutation is similar to updating pointers
to reorder a linked list, except that it is executed over
auxiliary state storing time-related data, rather than over real
state. This is why we refer to the idea as \emph{linking-in-time}.

%is that in addition to self and other histories that keep the
%real-time ordering of the program events, we also keep an auxiliary
%state that records a custom \emph{logical} ordering between these
%events. Conceptually, this differs from linearizability proofs as
%follows. In linearizability, the logical ordering of events is
%determined by the linearization points, which are given statically
%before the program is run, but then, their description often needs to
%rely on dynamic (i.e., run time) information, as pointed before. In
%contrast, in our approach, we embrace the fact that the logical
%ordering of events is a dynamic entity, \emph{and thus keep it as
%  such} in auxiliary state, allowing it to be modified \emph{in place}
%by auxiliary code.

Second, unlike in linearizability, where a concurrent program is
specified by an equivalent sequential one, our Hoare triples specify
programs \emph{in relation} to the behavior of the interfering
threads. Specifically, our Hoare triples scope over \emph{two}
\emph{local} variables $\histS$ (aka.~\emph{self}-variable) and
$\histO$ (aka.~\emph{other}-variable) that store the histories of the
events attributed to the specified program, and to its interfering
environment, respectively. The specifications can relate the events in
$\histS$ and $\histO$ to each other, and to the shared auxiliary
list of logical times described above. For example, an event with a
timestamp $t$ appearing in $\histS$ of a procedure $A$, models that a
call to $A$ was linearized at time $t$. But this timestamp may also be
seen as a pointer into the list of logical times. ``$A$'s linearization
point appearing in procedure $B$'' will be manifested by the auxiliary code of $B$
rearranging this list, to permute the node pointed by $t$. However, the
rearrangement does not change $A$'s ownership of the event occurring at $t$, as $t$
still appears in $\histS$ of $A$. The setup will enable us to specify
$A$ \emph{locally}, in terms of auxiliary state that $A$ manipulates,
rather than in terms of line numbers in the code of $B$. 

%auxiliary state. Also, auxiliary code can \emph{transfer} events from
%the shared components into $\histS$ and $\histO$. We exploit the
%latter to model the situations when a linearization point of a
%procedure $A$ appears in procedure $B$, as follows. The procedure $B$
%``allocates'' an event in shared auxiliary state to mark that the
%behavior of $A$ was linearized. The event is then ''collected'' by
%$A$'s auxiliary code, and transferred to $A$'s $\histS$.  This will
%enable us to specify $A$ \emph{locally}, in terms of auxiliary state
%that $A$ manipulates, rather than in terms of line numbers in the code
%of $B$.

%% \gad{Fix this paragraph! The transfer of tokens from $\histJ$ to
%%   $\hists$ has nothing to do with the linearization of the write
%%   event! The linearization of write events is always determined by
%%   $\jleq{L}$, which is joint state.}

% Treating time as space 
Encoding temporal information by way of mutable state, will allow us
to use, off-the-shelf, a variant of fine-grained concurrent separation
logic (FCSL)~\cite{Sergey-al:ESOP15} to verify example programs.  FCSL
has been implemented in the proof assistant Coq, and we are now
finishing the Coq mechanization of the example from this paper. 
%
While several recent Hoare logics have targeted concurrent programs
with non-thread-local and future-dependent linearization
points~\cite{Liang-Feng:PLDI13,Turon-al:ICFP13}, they only allowed to
establish a procedure's LP position based on the observations made
\emph{during} the procedure's symbolic execution, i.e., scoped within
its \emph{region}.
%
However, a number of modern concurrent data structures exhibit
executions whose linearization order can only be established
\emph{after} the involved overlapping procedure calls have
terminated~\cite{Dodds-al:POPL15,Jayanti:STOC05}. We call the
linearization points of such executions \emph{non-regional}, and our
method is the first that provides a logic for reasoning about
non-regional linearization points.

%FCSL-specific notion of auxiliary state to keep the behavior history
%of each thread. This auxiliary state is \emph{subjective}, which means
%that each procedure, in its pre- and post-condition, will have access
%to two \emph{local} variables: the \emph{self} variable $\histS$
%describing the history of the specified thread itself, and
%\emph{other} variable $\histO$ describing the history of the
%interfering threads collectively. Procedures can thus be specified in
%relation to their interfering environment. 
%%
%%The total history of the data structure is the disjoint union $\histS
%%\hunion \histO$.
%%
%Upon forking, the self history of the parent thread is split
%disjointly between the children, in the same way that in separation
%logic the heap of the parent is split disjointly between the
%children~\cite{ohe+rey+yan:csl01, Reynolds02}. This will provide us
%with a uniform treatment of spatial (heap) and temporal (histories)
%aspects of the algorithm~\cite{Sergey-al:ESOP15}.

%
%
%The second component of the idea, which is our main technical novelty,
%is that in addition to self and other histories that keep the
%real-time ordering of the program events, we also keep an auxiliary
%state that records a custom \emph{logical} ordering between these
%events. Conceptually, this differs from linearizability proofs as
%follows. In linearizability, the logical ordering of events is
%determined by the linearization points, which are given statically
%before the program is run, but then, their description often needs to
%rely on dynamic (i.e., run time) information, as pointed before. In
%contrast, in our approach, we embrace the fact that the logical
%ordering of events is a dynamic entity, \emph{and thus keep it as
%  such} in auxiliary state, allowing it to be modified \emph{in place}
%by auxiliary code.
%
%We find the uniform treatment of space and time afforded by FCSL in
%this setup to be particularly satisfying. Specifically, where
%separation logic provides effective means for reasoning about dynamic
%pointer \emph{linking in space}, in this paper, the same logical
%infrastructure models the dynamic ordering of linearization points of
%a data structure. We think this warrants being called \emph{linking in
%  time}.

%
%% \begin{comment}
%% In particular, we circumvent linearizability, in favor of Hoare-style
%% specifications in a variant of concurrent separation logic,
%% FCSL~\cite{LeyWild-Nanevski:POPL13,Nanevski-al:ESOP14}. We will rely
%% on two key features of FCSL to solve the problems. First, FCSL's
%% notion of auxiliary state will allow us to track the temporal
%% positioning of operations (an equivalent of linearization points in
%% linearizability reasoning) in a \emph{local} way. We do so by
%% utilizing FCSL abstractions that specify not only what the \emph{self}
%% thread (i.e., the thread being specified) does, but also what all the
%% \emph{other} threads in the system do collectively.

%% Second, FCSL abstracts over the specifics of how the state, real or
%% auxiliary, of the data structure is implemented. Any structure is
%% admissible, as long as it satisfies the algebraic axioms of a
%% \emph{Partial Commutative Monoid} (PCM). Heaps with the operation of
%% disjoint union are a particular PCM, taken as the default notion of
%% state in separation logic. But, we can take other PCMs, such as that
%% of \emph{histories} to keep track of the position of linearization
%% points. 

%% Thus, where separation logic provides effective means for
%% reasoning about pointer linking (and re-linking) in \emph{space}, in
%% FCSL, exactly the same logical infrastructure will provide us with
%% equally effective means for reasoning about linking (and re-linking)
%% in \emph{time}, which is precisely what the ordering (and re-ordering)
%% of linearization points of a data structure represents.
%% \end{comment}

%% Working with Hoare logic has a few additional benefits over
%% linearizability. First, where linearizability allows one program to be
%% \emph{replaced} by another, which can serve as the specification for
%% the first, here we use Hoare triples for specifications. This leads to
%% a kind of APIs and specifications for abstract data types which is
%% standard in sequential programming, but has not really been used in
%% concurrency. In particular, in this paper, we make the first proposal
%% of what we believe to be an API for concurrent snapshots, which is
%% \emph{principal}; that is, can be ascribed to different snapshot
%% algorithms, and can be used by clients without any modifications.
%% %
%% %\ab{At this point we don't know what principal API means.}
%% %The API is similar in spirit to the canonical APIs
%% %one considers for sequential structures, such as stacks or
%% %queues. 
%% Second, even if we replace a program by a simpler one in a client, we
%% still need to reason about the client. Linearizability offers no
%% particular help in this process,
%% %\an{E.g., we may need to introduce
%% %  auxiliary state into the simpler program, but that may destroy the
%%  % connectons with the original program}, 
%% %
%% whereas our approach does, by providing principal APIs.
%% %
%% %
%% %Lastly, our approach immediately lends itself to reasoning about
%% %higher-order programs, whereas scaling linearizability to higher-order
%% %programs has been tricky, and has so far been completed only for very
%% %specific scenarios (CITE some Gotsman stuff)\gad{Which?}\an{Maybe
%% %  remove, as now we're not mentioning higher order stuff anywhere.}

%% %\an{May need to temper this statement, to not incense CaReSL
%% %  guys.}\is{No, we don't have to as they do the refinement reasoning,
%% %  which is the subject of the listed above problem with giving
%% %  meaningful specs. This can go to related work.}\an{This sentence
%% %  would be stronger, if we actually considered some higher-order
%% %  client. It shouldn't be hard to concoct one in the section on
%% %  clients.}

We illustrate the method by applying it to a snapshot algorithm of
Jayanti~\cite{Jayanti:STOC05}, whose linearization points depend on
dynamic information in an intricate non-regional way. We will also
show how the auxiliary state and code should be designed to provide
local specifications for this algorithm and discuss some aspects of
our proof of correctness. The complete datails of our development be
can consulted from our draft~\cite{DelbiancoSNB16+arxiv16} and its
accompanying Coq development~\cite{CoqFiles}.

%We make an
%interesting parallel with linearizability, as this auxiliary state has
%to keep track of both beginning and ending times of an operation, for
%the purposes of the proof, although one of the endpoints can be
%omitted in the specs, which we present in the abstract form in Section
%4, along with the commentary of the formal proof. 
%
%This is the first proof of Jayanti's algorithm in a formal program
%logic. 
%
%Moreover, the proof is mechanized in in Coq.
%
%\an{Maybe not in Coq.}
%
%% In Section 4, we illustrate that the same specs can be ascribed to at
%% least one more snapshot implementation.
%
%% We discuss the implications of the design to client-side reasoning
%% (Section~\ref{sec:clients}) and conclude with the related work
%% (Section~\ref{sc:related}).
%
%before concluding (Section~6).
%
%Following the old adage that a method is a trick which worked at least
%twice, this lends credence to the claim that our snapshot API is
%canonical. We illustrate how the specs works in a several simple
%client program scenarios.
%
%\gad{WRT the Coq mechanization: Note that PODC's submission page
%  expects a .pdf file. Then if we provide code, it will have to be
%  available on-line. That could save us some time to try to push it
%  after the deadline but, it would be a risky enterprise. I'd rather
%  go safe and claim the mechanization is a work in progress. There is
%  no rebuttal period to argue back that we have finished it either.}

%\gad{ We should remember to point out the fact that unlike
%  linearizability proofs, we do not track the timestamp of all events,
%  but rather a few selected events. Moreover, our specs only need to
%  expose the timestmaps of atomic write events in the histories, as
%  the only scanner timestamp that we are interested in, the
%  linearization point of the last scanner, is stored only in the
%  internal auxiliary state.  We need to spin this fact in our favor
%  here, and stress it later in Section~\ref{sc:formal} in further
%  detail.}

\section{Introduction}
\label{sc:intro} 
   
Formal verification of concurrent objects commonly requires reasoning
about linearizability~\cite{Herlihy-Wing:TOPLAS90}. This is a standard
correctness criterion whereby a concurrent execution of an object's
procedures is proved equivalent, via a simulation argument, to some
sequential execution. The clients of the object can be verified under
the sequentiality assumption, rather than by inlining the procedures
and considering their interleavings. Linearizability is often
established by describing the \emph{linearization points} (LP) of the
object, which are points in time where procedures take place,
\emph{logically}.  In other words, even if the procedure physically
executes across a time interval, exhibiting its linearization point
enables one to pretend, for reasoning purposes, that it occurred
instantaneously; hence, an interleaved execution of a number of
procedures can be reduced to a sequence of instantaneous events.

%This simplifies the reasoning about client programs, which can treat
%the physical operation as if it were indivisible, i.e.~\emph{atomic}.

However, reasoning with linearizability, and about linearization
points, can be tricky. Many times, a linearization point of a
procedure is not \emph{local}, but may appear in another procedure or
thread. Equally bad, a linearization points' place in time may not be
determined statically, but may vary based on the past, and even
future, \emph{run-time} information. This complicates the simulation
arguments, leading to unwieldy formal logical proofs.

%n a setting of formal logical proofs, often requires formal
%abstractions such as speculation or prophecy
%variables~\cite{Abadi-Lamport:LICS88}, that make proofs unwieldy.

\begin{comment}
First, one has to decide just how to shrink to
a single point the whole physical interval in which the operation
executed. This allows for some leeway. For example, operations that
execute during overlapping time intervals can be assigned LPs that are
ordered arbitrarily. An operation A can be ``linearized'' before B,
despite B beginning before A physically begins, and ending before A
physically ends, as long as A and B overlapped.
%
Worse, the position of a linearization point for a certain procedure
may not be determined \emph{locally}, but may appear in another
procedure or thread. Moreover, this position may vary depending on
past, \emph{or even future}, run-time events. For instance, a
linearization point of a procedure A, may appear in the procedures B
or C, depending on some run-time value. We will illustrate this
possibility in Section~\ref{sc:overview}.
%% \is{The above is a soooper-vague example. I'd recommend to show
%%   something concrete here, for instance the simple readPair program,
%%   outlining the difficulty in reasoning.}  
%% %
%% \an{Indeed, I'd like an example. But, readpair may be too complicated
%%   to put here, no?  Moreover, if I recall correctly, it's
%%   linearization points were all in the same procedure. Why not just
%%   put a forward pointer here, and say here that an example will be
%%   given in Section 2? Alternatively, we introduce readpair here, but
%%   say that in Section 2 we'll show an example where distribution of
%%   linearization points is even trickier. However, both algorithms will
%%   be assignable the same spec.}  
%% %
%% \an{Can somebody please type up readpair, so we can see how it looks
%%   in the intro? We need to make good progress on this paper before we
%%   skype next.} \gad{I'm on it}
\end{comment}
  
\begin{comment}
\input{readpair}

Fine-grained {\it memory snapshots}~\cite[Chapter~4.3]{HerlihyS+art08}
constitute a good example of a concurrent data-structure where the
linearization points depend on complex, non local, interaction between
methods. A memory snapshot is a collection of values read from
independent memory locations, e.g. an array, that co-existed at a
given point of time.  \gad{I'm not quite sure about how read-pair fits
  here. Certainly read-pair and jayanti have non-fixed LPs, is this
  true in general?}

Figure~\ref{fig:readpair} presents the \code{scan} method for the
\emph{atomic pair snapshot} data
structure~\cite{QadeerST+tr09,LiangF+pldi13,SergeyNB+esop15}.
%
The latter consists of a pair of pointers, $x$ and $y$, pointing to
tuples $(c_x, v_x)$ and $(c_y, v_y)$, respectively. The components
$c_x$ and $c_y$ of type $A$ represent the accessible contents of $x$
and $y$, that may be read and updated by the client. The components
$v_x$ and $v_y$ are, respectively, the ``version numbers'' for $x$ and
$y$. They are internal to the structure and not directly accessible by
its clients.

The data structure interface exports three methods: \code{writeX},
\code{writeY} and \code{readPair}, which returns the snapshot. The
latter is the most interesting of the three: it is clear that a {\it
  na\"{i}ve} implementation which first reads $x$, then $y$, and
returns the pair $(c_x, c_y)$ does not guarantee that $c_x$ and $c_y$
ever appeared together in the data-structure. Instead, after having
read the two components, \code{readPair} performs a second atomic read
of one of the components, \code{readX} in line~4, only to check in
line~5 that its version number $tx$ has not changed. If this is the
case, the pair $(c_x,c_y)$ is still a valid snapshot and hence it is
returned (line~6) else \code{readPair} loops, re-starting the whole
process.

\code{ReadPair} has a {\it future-dependent} linearization point: if
the check at line~5 holds, then the linearization point corresponds to
the \code{readY} call on line~3. Moreover, it is not a realistic
snapshot scan operation: the unbounded, interference-dependent loop
doesn't provide the clients with meaningfull bounds on the algorithm's
complexity. \gad{Meh. I don't know right now how to sell this better.}

In Section~\ref{sc:overview}, we present Jayanti's {\it optimal}
snapshot algorithm\cite{Jayanti:STOC05}, whose worst-case time
complexity is $\mathcal{O}(m)$, with $m$ being the size of the
object. This algorithm, however, presents a more intricate
interference pattern, and its LPs will turn out to be not only
non-fixed but also non-local.

\gad{Should we say here that previous/most approaches to reason with
  non-fixed linearization point do so with a dedicated
  meta-theory~\cite{QadeerST+tr09,LiangF+pldi13}?}

 \gad{Also, I don't like how the \code{readPair} fits here so far. We
   are ignoring the fact that you guys've already verified it in
   FCSL.}

\end{comment}
 
This paper presents a novel specification and verification method for
concurrent objects, based on Hoare logic. It achieves the same goal as
linearizability; that is, client proofs can reason out of Hoare
triples of the object's procedures, rather than inline the procedures
and consider all interleavings. The method improves on linearizability
by offering natural abstractions for specifying dynamic and non-local
linearization points, based on familiar concepts from Hoare logics for
shared-memory concurrency. It also permits better \emph{information
  hiding}, allowing us to expose to clients less information about the
internals of the object compared to linearizability.

%For example, linearizability proofs by definition have to operate on
%the procedure's beginning and ending times. Our proofs do the same
%\emph{internally}, but most of that information can be hidden from the
%clients, which only need to see the \emph{relative logical ordering}
%of events, and then only of \emph{some} of the events.

More specifically, the method consists of two
components. First, we use shared \emph{auxiliary
  state}~\cite{Owicki-Gries:CACM76} to record, as a list of timed
events (e.g., writes occurring at a given time), the logical order in which the object's procedures are
perceived to execute, each instantaneously. Tracking this time-related
information through state enables us to specify the linearization
points \emph{dynamically}. In particular, we can use auxiliary
  code to mutate the logical order in place, thereby permuting
the sequencing of the procedures, as may be needed when some run-time
event occurs. This mutation is similar to updating pointers
to reorder a linked list, except that it is executed over
auxiliary state storing time-related data, rather than over real
state. This is why we refer to the idea as \emph{linking-in-time}.

%is that in addition to self and other histories that keep the
%real-time ordering of the program events, we also keep an auxiliary
%state that records a custom \emph{logical} ordering between these
%events. Conceptually, this differs from linearizability proofs as
%follows. In linearizability, the logical ordering of events is
%determined by the linearization points, which are given statically
%before the program is run, but then, their description often needs to
%rely on dynamic (i.e., run time) information, as pointed before. In
%contrast, in our approach, we embrace the fact that the logical
%ordering of events is a dynamic entity, \emph{and thus keep it as
%  such} in auxiliary state, allowing it to be modified \emph{in place}
%by auxiliary code.

Second, unlike in linearizability, where a concurrent program is
specified by an equivalent sequential one, our Hoare triples specify
programs \emph{in relation} to the behavior of the interfering
threads. Specifically, our Hoare triples scope over \emph{two}
\emph{local} variables $\histS$ (aka.~\emph{self}-variable) and
$\histO$ (aka.~\emph{other}-variable) that store the histories of the
events attributed to the specified program, and to its interfering
environment, respectively. The specifications can relate the events in
$\histS$ and $\histO$ to each other, and to the shared auxiliary
list of logical times described above. For example, an event with a
timestamp $t$ appearing in $\histS$ of a procedure $A$, models that a
call to $A$ was linearized at time $t$. But this timestamp may also be
seen as a pointer into the list of logical times. ``$A$'s linearization
point appearing in procedure $B$'' will be manifested by the auxiliary code of $B$
rearranging this list, to permute the node pointed by $t$. However, the
rearrangement does not change $A$'s ownership of the event occurring at $t$, as $t$
still appears in $\histS$ of $A$. The setup will enable us to specify
$A$ \emph{locally}, in terms of auxiliary state that $A$ manipulates,
rather than in terms of line numbers in the code of $B$. 

%auxiliary state. Also, auxiliary code can \emph{transfer} events from
%the shared components into $\histS$ and $\histO$. We exploit the
%latter to model the situations when a linearization point of a
%procedure $A$ appears in procedure $B$, as follows. The procedure $B$
%``allocates'' an event in shared auxiliary state to mark that the
%behavior of $A$ was linearized. The event is then ''collected'' by
%$A$'s auxiliary code, and transferred to $A$'s $\histS$.  This will
%enable us to specify $A$ \emph{locally}, in terms of auxiliary state
%that $A$ manipulates, rather than in terms of line numbers in the code
%of $B$.

%% \gad{Fix this paragraph! The transfer of tokens from $\histJ$ to
%%   $\hists$ has nothing to do with the linearization of the write
%%   event! The linearization of write events is always determined by
%%   $\jleq{L}$, which is joint state.}

% Treating time as space 
Encoding temporal information by way of mutable state, will allow us
to use, off-the-shelf, a variant of fine-grained concurrent separation
logic (FCSL)~\cite{Sergey-al:ESOP15} to verify example programs.  FCSL
has been implemented in the proof assistant Coq, and we are now
finishing the Coq mechanization of the example from this paper. 
%
While several recent Hoare logics have targeted concurrent programs
with non-thread-local and future-dependent linearization
points~\cite{Liang-Feng:PLDI13,Turon-al:ICFP13}, they only allowed to
establish a procedure's LP position based on the observations made
\emph{during} the procedure's symbolic execution, i.e., scoped within
its \emph{region}.
%
However, a number of modern concurrent data structures exhibit
executions whose linearization order can only be established
\emph{after} the involved overlapping procedure calls have
terminated~\cite{Dodds-al:POPL15,Jayanti:STOC05}. We call the
linearization points of such executions \emph{non-regional}, and our
method is the first that provides a logic for reasoning about
non-regional linearization points.

\begin{comment}
Without going into details of separation logic, whose understanding is
not required for this paper, we mention that we inherit from
separation logic, and generalize, two important mechanisms. First,
where separation logic allows framing of heaps (i.e., extending the
heap that the procedure manipulates without changing the underlying
proof), we will allow framing of time (i.e., extending the history
that the procedure manipulates), which will lead to particularly
concise specifications.
%and we illustrate in Section~\ref{sec:main}
%where this is useful. 
Second, we inherit the mechanism for reasoning about programs with
nested parallel composition (i.e., dynamic, well-bracketed, forking
and joining). Such programs are easy for us to handle, but are not
often considered in the work on linearizability, where programs are
typically a \emph{top-level} parallel composition of a fixed number of
\emph{sequential} threads.
\end{comment}

%FCSL-specific notion of auxiliary state to keep the behavior history
%of each thread. This auxiliary state is \emph{subjective}, which means
%that each procedure, in its pre- and post-condition, will have access
%to two \emph{local} variables: the \emph{self} variable $\histS$
%describing the history of the specified thread itself, and
%\emph{other} variable $\histO$ describing the history of the
%interfering threads collectively. Procedures can thus be specified in
%relation to their interfering environment. 
%%
%%The total history of the data structure is the disjoint union $\histS
%%\hunion \histO$.
%%
%Upon forking, the self history of the parent thread is split
%disjointly between the children, in the same way that in separation
%logic the heap of the parent is split disjointly between the
%children~\cite{ohe+rey+yan:csl01, Reynolds02}. This will provide us
%with a uniform treatment of spatial (heap) and temporal (histories)
%aspects of the algorithm~\cite{Sergey-al:ESOP15}.

%
%
%The second component of the idea, which is our main technical novelty,
%is that in addition to self and other histories that keep the
%real-time ordering of the program events, we also keep an auxiliary
%state that records a custom \emph{logical} ordering between these
%events. Conceptually, this differs from linearizability proofs as
%follows. In linearizability, the logical ordering of events is
%determined by the linearization points, which are given statically
%before the program is run, but then, their description often needs to
%rely on dynamic (i.e., run time) information, as pointed before. In
%contrast, in our approach, we embrace the fact that the logical
%ordering of events is a dynamic entity, \emph{and thus keep it as
%  such} in auxiliary state, allowing it to be modified \emph{in place}
%by auxiliary code.
%
%We find the uniform treatment of space and time afforded by FCSL in
%this setup to be particularly satisfying. Specifically, where
%separation logic provides effective means for reasoning about dynamic
%pointer \emph{linking in space}, in this paper, the same logical
%infrastructure models the dynamic ordering of linearization points of
%a data structure. We think this warrants being called \emph{linking in
%  time}.

%
%% \begin{comment}
%% In particular, we circumvent linearizability, in favor of Hoare-style
%% specifications in a variant of concurrent separation logic,
%% FCSL~\cite{LeyWild-Nanevski:POPL13,Nanevski-al:ESOP14}. We will rely
%% on two key features of FCSL to solve the problems. First, FCSL's
%% notion of auxiliary state will allow us to track the temporal
%% positioning of operations (an equivalent of linearization points in
%% linearizability reasoning) in a \emph{local} way. We do so by
%% utilizing FCSL abstractions that specify not only what the \emph{self}
%% thread (i.e., the thread being specified) does, but also what all the
%% \emph{other} threads in the system do collectively.

%% Second, FCSL abstracts over the specifics of how the state, real or
%% auxiliary, of the data structure is implemented. Any structure is
%% admissible, as long as it satisfies the algebraic axioms of a
%% \emph{Partial Commutative Monoid} (PCM). Heaps with the operation of
%% disjoint union are a particular PCM, taken as the default notion of
%% state in separation logic. But, we can take other PCMs, such as that
%% of \emph{histories} to keep track of the position of linearization
%% points. 

%% Thus, where separation logic provides effective means for
%% reasoning about pointer linking (and re-linking) in \emph{space}, in
%% FCSL, exactly the same logical infrastructure will provide us with
%% equally effective means for reasoning about linking (and re-linking)
%% in \emph{time}, which is precisely what the ordering (and re-ordering)
%% of linearization points of a data structure represents.
%% \end{comment}

%% Working with Hoare logic has a few additional benefits over
%% linearizability. First, where linearizability allows one program to be
%% \emph{replaced} by another, which can serve as the specification for
%% the first, here we use Hoare triples for specifications. This leads to
%% a kind of APIs and specifications for abstract data types which is
%% standard in sequential programming, but has not really been used in
%% concurrency. In particular, in this paper, we make the first proposal
%% of what we believe to be an API for concurrent snapshots, which is
%% \emph{principal}; that is, can be ascribed to different snapshot
%% algorithms, and can be used by clients without any modifications.
%% %
%% %\ab{At this point we don't know what principal API means.}
%% %The API is similar in spirit to the canonical APIs
%% %one considers for sequential structures, such as stacks or
%% %queues. 
%% Second, even if we replace a program by a simpler one in a client, we
%% still need to reason about the client. Linearizability offers no
%% particular help in this process,
%% %\an{E.g., we may need to introduce
%% %  auxiliary state into the simpler program, but that may destroy the
%%  % connectons with the original program}, 
%% %
%% whereas our approach does, by providing principal APIs.
%% %
%% %
%% %Lastly, our approach immediately lends itself to reasoning about
%% %higher-order programs, whereas scaling linearizability to higher-order
%% %programs has been tricky, and has so far been completed only for very
%% %specific scenarios (CITE some Gotsman stuff)\gad{Which?}\an{Maybe
%% %  remove, as now we're not mentioning higher order stuff anywhere.}

%% %\an{May need to temper this statement, to not incense CaReSL
%% %  guys.}\is{No, we don't have to as they do the refinement reasoning,
%% %  which is the subject of the listed above problem with giving
%% %  meaningful specs. This can go to related work.}\an{This sentence
%% %  would be stronger, if we actually considered some higher-order
%% %  client. It shouldn't be hard to concoct one in the section on
%% %  clients.}

We illustrate the method by applying it to a snapshot algorithm of
Jayanti~\cite{Jayanti:STOC05} (Section~\ref{sc:overview}), whose
linearization points depend on dynamic information in an intricate
non-regional way. We show how the auxiliary state and code should be
designed to provide local specifications for this algorithm
(Sections~\ref{sc:formal},\ref{sc:implementation}), and sketch some
aspects of our version of the proof (Section~\ref{sc:implementation}).
%We make an
%interesting parallel with linearizability, as this auxiliary state has
%to keep track of both beginning and ending times of an operation, for
%the purposes of the proof, although one of the endpoints can be
%omitted in the specs, which we present in the abstract form in Section
%4, along with the commentary of the formal proof. 
%
%This is the first proof of Jayanti's algorithm in a formal program
%logic. 
%
%Moreover, the proof is mechanized in in Coq.
%
%\an{Maybe not in Coq.}
%
%% In Section 4, we illustrate that the same specs can be ascribed to at
%% least one more snapshot implementation.
%
We discuss the implications of the design to client-side reasoning
(Section~\ref{sec:clients}) and conclude with the related work (Section~\ref{sc:related}).
%
%before concluding (Section~6).
%
%Following the old adage that a method is a trick which worked at least
%twice, this lends credence to the claim that our snapshot API is
%canonical. We illustrate how the specs works in a several simple
%client program scenarios.
%
%\gad{WRT the Coq mechanization: Note that PODC's submission page
%  expects a .pdf file. Then if we provide code, it will have to be
%  available on-line. That could save us some time to try to push it
%  after the deadline but, it would be a risky enterprise. I'd rather
%  go safe and claim the mechanization is a work in progress. There is
%  no rebuttal period to argue back that we have finished it either.}

%\gad{ We should remember to point out the fact that unlike
%  linearizability proofs, we do not track the timestamp of all events,
%  but rather a few selected events. Moreover, our specs only need to
%  expose the timestmaps of atomic write events in the histories, as
%  the only scanner timestamp that we are interested in, the
%  linearization point of the last scanner, is stored only in the
%  internal auxiliary state.  We need to spin this fact in our favor
%  here, and stress it later in Section~\ref{sc:formal} in further
%  detail.}
